\section{Results}

This chapter emphasized the empirical evaluation of the performance of the most used feature effects techniques: \gls{ALE}, \gls{SHAP}, \gls{ME}, and \gls{PD} plots. The primary objective is to discern the differences in the explanations provided by these techniques in terms of the global effects of variables compared to the true variable effects inherent in the data-generating process. The introduced \gls{ABX} metric serves as the benchmark for this evaluation. Lower \gls{ABX} values are preferable as they indicate a closer alignment of the explainable technique's output with the true variable effects along all the variable ranges.

Two distinct types of supervised models were considered: \gls{RF}s and \gls{NN}s, with hyperparameters optimized through a cross-validation process under various conditions. It is crucial to acknowledge that the conditions under which models are applied can inherently influence their outputs and, consequently, the interpretations derived from model explanation techniques. Nevertheless, we applied both models and explanation techniques consistently across these conditions to ensure that our evaluation remains unbiased. Moreover, in all tested conditions, both models demonstrated robust performance, with a \gls{RMSD} close to the standard deviation of the theoretical noise added to the target variable. This indicates that the models were effectively capturing underlying patterns in the data.

The results of the experiments are presented in Table \ref{tbl:exp1_v1} for $x_1$ and in Table \ref{tbl:exp1_v2} for $x_2$. Only values for \(k\) equal to 10 are presented, as there is no significant difference compared to the \\\(k\) equal to 50. Both models, \gls{NN} and \gls{RF}, performed well when measuring the \gls{RMSD}. The \gls{RMSD} values were found to be very close to the standard deviation of the artificial noise (0.1) added to the target variable. This suggests that, to a considerable extent, the models are effectively capturing the underlying relationship between the predictors \(x1, x2\) and the response variable \(y\) . 

\input{tables/exp1_exp2}

Examining Table \ref{tbl:exp1_v1}, it is evident that \gls{ALE} technique outperforms the other techniques in all scenarios where data is dependent. In the hypothetical scenario of independent data - a condition that may diverge from real-world situations, where data typically exhibits some level of correlation - all techniques yield comparably satisfactory results for the RF model, with \gls{ME} producing the best values. However, while \gls{ME} most accurately captures the effects of variable 1 ($x_1$), it produces suboptimal results for variable 2 ($x_2$) (Table \ref{tbl:exp1_v2}), which has no effect on $y$ in this independent scenario. It is probable that some effect was assigned to the $x_2$ through $x_1$ even variables being independent of each other. This highlights the pitfalls of using the data joint distributions without interventions when explaining models under independent data.

In the same independent scenario, permutation-based techniques (\gls{PD} and \gls{SHAP}), which perform interventions (albeit at the cost of extrapolation), produce commendable results for the \gls{RF} model but not for the \gls{NN} model. Initially, this discrepancy may be attributed to the high flexibility of \gls{NN}s \cite{Grinsztajn2022WhyData}, which can potentially yield many functions consistent with the observed data but divergent from the true data-generating process. This flexibility can lead to a mismatch between explanations derived from the \gls{NN} model and the actual data-generating process. 

Upon closer examination, however, \gls{ALE} and \gls{ME}, which unlike \gls{ME} plots and \gls{SHAP} do not extrapolate the training data, exhibit comparably low \gls{ABX} across both \gls{RF} and \gls{NN} models within the independent scenario. Consequently, the discrepancy from \gls{RF} to \gls{NN} in \gls{ME} and \gls{PD} outputs, indeed, suggests that the extrapolation can be problematic even in independent scenarios when explaining highly complex models such as \gls{NN}s. \gls{NN} may inadvertently yield unusual predictions outside of the training data, thus compromising their ability to capture the effect even from the independent features due to the model's complex behavior.

Conversely, step-wise algorithms, such as RF, tend to exhibit greater stability in their predictions owing to their construction from multiple decision trees, which individually handle variations in data in a more controlled manner.

The quantity of data points significantly affects the outcomes in the independent scenario. Generally, an increase in data points enhances the performance of all examined techniques within both algorithms. This trend is also observable in the dependent scenario for the \gls{ALE} technique, which slightly improves \gls{ABX} as the number of data points increases. Although a similar improvement can be observed for other techniques, it is not enough to decrease the \gls{ABX} to levels comparable to those achieved by \gls{ALE} technique.

The results for the \gls{ALE} technique is even more favorable when considering the variable $x_2$ as shown in Table \ref{tbl:exp1_v2}. In the independent scenario, all techniques failed to assign no effect to $x_2$ for both \gls{NN} and \gls{RF} models. In other scenarios, where the data are dependent, the \gls{ALE} technique achieves better results, while the other techniques exhibit higher \gls{ABX} values. 




