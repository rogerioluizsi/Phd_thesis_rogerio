\chapter{Introduction}
\label{contextualizacao}
This thesis introduces and implements global \gls{XAI} techniques to enhance \gls{KDD}, particularly within the educational domain. By integrating methodologies from both Machine Learning (ML) and Educationâ€”fields distinguished by their unique terminologies - this work relaxes the use of certain terms for clarity. Here, 'variable', 'feature', and 'predictor' are used interchangeably to represent individual, measurable attributes of the phenomena observed in the datasets. Similarly, 'label' 'target', 'dependent variable,' and 'outcome' denote the variables whose values the models aim to predict. Additionally, the term 'marginal effects' is utilized in two contexts. In econometrics, it indicates the incremental effect, while in statistics, it pertains to the probability distribution of a variable. Contextual clarifications are provided throughout to avoid misinterpretations.

\section{Contextualization}

In modern society, data is a critical resource for guiding human decision-making processes. More recently, with the technological advances of the twentieth century, our capability to store, process, and analyze large volumes of data has put forward the data potential to enhance human activities \cite{Provost2013DataMaking}. As we transition into this new era characterized by data ubiquity, emergent paradigms in data analysis have arisen to meet the challenges and opportunities presented by this voluminous and complex data landscape. 

In 2001, Breiman called for the use of an algorithm approach as a more accurate and informative alternative to the use of data to solve problems \cite{Breiman2001StatisticalAuthor}. The algorithmic modeling he refers to is ML, which, unlike traditional approaches which adjust data for a predefined model, learns empirically from data to estimate functions for making predictions on new data. According to Breiman, ML tools facilitate a move away from exclusive reliance on parametric models, adopting a more diverse set of tools. This approach could enable researchers to move beyond confirmatory research based on theory models and also allow them to derive new theories directly from data \cite{Molina2019AnnualSociology}.

In supervised ML \cite{Vapnik1999AnTheory}, models are iteratively optimized to minimize out-of-sample prediction error, a focus that diverges from disciplines more concerned with understanding the underlying data-generating processes \cite{Mullainathan2017MachineApproach}. However, \cite{Shmueli2010ToPredict, Zhao2021CausalModels} argue that a model exhibiting both strong predictive performance and consistent assumptions closely approximates the underlying natural laws governing the data. This dual focus not only highlights the significance of ML's predictive capabilities but also the critical importance of ensuring that models align with real-world phenomena. Further reinforcing this notion is the stance taken by \cite{Cao2009IntroductionMining}, who stresses the importance of aligning data mining models with complex real-world challenges. Cao advocates for the integration of domain-specific knowledge throughout the entire \gls{KDD} process, a strategy that promises to deliver more reliable and actionable insights.

Nonetheless, the emphasis on predictive performance in \gls{ML} has prompted researchers to adopt increasingly complex models, often at the expense of interpretability. For instance, the coefficients in additive linear models or the rules derived from decision trees offer straightforward interpretability, explicitly mapping input features to model outputs\cite{molnar2019}. In contrast, opaque models like neural networks and ensemble methods, though potentially superior in prediction, do not readily reveal the mechanisms relating input features to outcomes\cite{Linardatos2021ExplainableMethods}. The complexity of these models poses significant challenges for interpretation within a \gls{KDD} process, especially when seeking scientific insights and explanations for wrong decisions made, particularly before the Justice. 

Within this context, and given the widespread adoption of \gls{ML} in many tasks,  the field of \gls{XAI} has quickly become an important focus within the larger field of \gls{ML}. \gls{XAI} aims to explain the reasoning and decision-making processes of these models in a human-understandable manner \cite{Miller2019ExplanationSciences}. These explanations are valuable not only for applications aimed at deriving insights from data but also for those whose primary objective is prediction. For instance, while categorizing a patient's health status in a hospital or predicting a student's likelihood of dropping out is beneficial, understanding the factors driving these predictions can significantly enhance the utility of the model by facilitating targeted interventions \cite{Razavian2015Population-levelFactors, Pellagatti2021GeneralizedDropout, Berens2019EarlyMethods}.  Furthermore, the transparency of \gls{ML} models in sectors like criminal justice \cite{Wang2023InPrediction} and finance \cite{Bussmann2021ExplainableManagement, Chen2023Globally-ConsistentEvaluation} are increasingly mandated by legal and ethical considerations. 






