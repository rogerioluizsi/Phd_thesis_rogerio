\chapter{Conclusion}
\label{chap: conclusion}

This chapter presents the concluding remarks of this thesis, emphasizing its contributions to the fields of \gls{XAI} and supervised learning, clearly illustrated within the context of \gls{EDM}. Moreover, this chapter delineated the inherent limitations encountered during the research and discussed a few themes that future works should focus on.

\section{Concluding remarks}

The application of \gls{ML} in data analysis represents a significant research opportunity. The recent growth in data collection enables the use of \gls{ML} techniques that are versatile enough to capture intricate data relationships. However, using \gls{ML} for this purpose presents challenges, as these techniques may lack transparency in how they adjust to the data to make predictions. Developing model explanations can help overcome this issue, enhancing knowledge extraction from complex data and supporting more informed, strategic data-driven decisions.

The objective of this thesis is to evaluate and propose methods to derive global explanations in the context of supervised \gls{ML}. Specifically, it focuses on providing a more unbiased and isolated understanding of feature roles in predictive models. 

With advancements in \gls{ML} and \gls{XAI}, many researchers increasingly use these tools to extract insights from data. In the educational domain, this kind of research is widely employed to identify significant predictor variables aiming to foster the educational ecosystem and advance the field of \gls{EDM}. However, a comprehensive literature review reveals that conventional \gls{XAI} methods used in \gls{EDM} have interpretability limitations when applied to dependent data, frequent situation in educational contexts, which is a key motivation for using \gls{ML} models. These methods often rely on assumptions that are frequently unmet, limiting the insights gained from the data. 

The problem arises from the tendency of these methods to "extrapolate" existing data relationships when computing the contribution of variables within a predictive function. Additionally, existing techniques often do not align with the primary objectives of educational practitioners when analyzing effect sizes in statistical analysis. For instance, the score-based explanations often focus on a variable's impact on model performance rather than on its directly contribution to the model's predictions.

The \gls{ALE} technique have been proposed as a prominent \gls{XAI}  technique that can minimize the extrapolation problem when computing feature effects. However, there is a limited assessment of these \gls{ALE} capabilities when compared to other widely used explanation techniques. Additionally, \gls{ALE} has only been defined for visualizing feature effects with limited discussion of use of \gls{ALE} to derive score-based explanations. 

Witin this context, this thesis goes to this problem formulating two main research questions:


RQ1 - How do widely used feature effects techniques compare with \gls{ALE} in accurately identifying true feature effects considering different inter-data dependencies?

RQ2 - How effectively can score-based explanations derived from the \gls{ALE} framework report individual and isolated attribution of the features in terms of their magnitude and direction compared to existing methods?

In response to RQ1, Chapter \ref{chap:AssALE} benchmarked the most used explainable techniques on \gls{EDM} and \gls{ALE}, a recent contribution of \gls{XAI} that employes some constraints to avoid data extrapolation. The benchmarking indentify \gls{ALE} as the most suitable technique to report features effects when features are correlated. Building on this finding, Chapter \ref{chapter4} answer RQ2 by proposing a set of \gls{ALE}-based metrics to enhance the clarity and utility of the supervised model explanations focused on overall variables effect size. Chapter \ref{chap: realApplication} then demonstrated the practical application of these metrics in a real-world educational context.

\subsection{Summary of contributions}

\subsubsection{A benchmarking of feature effects techniques}

Answering RQ1 and aiming to provide empirical evidence regarding the robustness of \gls{ALE} as compared to baseline methods in dependent data, a benchmarking of the feature effect technique was established. To the best of our knowledge, this is the first quantitative comparison of the accuracy of \gls{PD} plots and \gls{ALE} against a ground truth. Also, other techniques widely used in literature \gls{ME} and \gls{SHAP} were included, enhancing the benchmarking for a broader comparison. A new comparison metric, the \gls{ABX}, was introduced to measure the area between the true and explained features.

The \gls{ALE} outperformed in accurately recovering the feature effects in all scenarios under dependent data. Also, the experiments highlight the potential risk of explaining highly flexible algorithms, such as neural networks, using techniques that extrapolate the manifold even on independent datasets. This phenomenon is already known and has also been simulated in this thesis (Chapter \ref{chapter4}, Figure \ref{fig:edu_data}), but, to our knowledge, it has not yet been identified in empirical experimentation. These results may aid the \gls{XAI} field, which can use the benchmarking framework as well for applied researchers, especially on \gls{EDM}, that could identify the pitfalls of the most currently used \gls{XAI} techniques to derive insights about the data.

\subsubsection{New scores of feature effects size}

Motivated by the robustness of \gls{ALE} and the limitations of existing scores of feature importance, this contribution introduces four new model-agnostic measures of variable effect size based on \gls{ALE}. These measures are designed for enhanced interpretability of feature roles in predictions, especially in scenarios involving dependent data. Three of these metrics offer distinct single-explanation perspectives, elucidating the extent and direction of feature effects in relation to the target variables. Each metric presents a unique interpretation, adding depth to the understanding of feature influence. The fourth metric offers a normalized ranking of feature impact, facilitating their comparison across different datasets and models. 

In evaluations, these features exhibit similar or superior performance compared to existing metrics in the \gls{XAI} literature, proving effective in identifying key variables in both synthetic and real datasets. The metrics can be employed either in cross-validation settings for more robust estimates or bootstrap, allowing yield confident intervals to account for variability and uncertainty inherent to the data and the model.

Calculating scores using \gls{ALE} introduces specific limitations inherent to model-agnostic methods. The generalizability of the results largely depends on how representative the sample is of the population. Furthermore, an important aspect of \gls{ALE} is its computation by segments rather than analyzing the entire dataset at once. As a result, it is necessary to consider the actual data sample used along with how each provided metric is computed for an appropriate interpretation and generalization of the explanation outputs. Despite this limitation, the local nature of \gls{ALE} has partial benefits for the purpose of the metrics. The \gls{ALE} ensures that explanations remain faithful to the relationships in the data. Additionally, \gls{ALE} enables the computation of the isolated variable effects and their interactions within this interval.

However, relying solely on one score to represent the entire distribution produced by the \gls{ALE} function may be problematic and conceal important aspects of the shape of variable effects, especially in cases where they are noisy or have been calculated based on a limited number of data points. Following the \gls{ALE} limitation, the metrics cannot also be computed for categorical variables without order relation. Finally, although \gls{ALE} permits the computation of interaction effects, which have also been defined in the context of scores in this thesis, the empirical experimentation focused solely on assessing them by computing the main effects of feature size.

\subsubsection{A empirical trend analysis of Brazilian secondary schools determinants}

To demonstrate the usefulness and the meaningful of the proposed scores in the exploration of educational data, an empirical case study was presented. The real scenario seeks to identify and track the determinants of Brazilian public education from 2009 to 2019. To the best of our knowledge, we are the first to explore the impact of contextual features on educational outcomes through supervised learning over time. Previous studies have handled this problem only at a single point in time. While \cite{Franco2020UsandoAnos} used multiple years of ENEM data in Brazil to conduct similar research, their work did not aim to make results comparable, which does not allow for tracking the feature effects size over time.

Moreover, the defined process is also a contribution of this thesis to researchers interested in conducting repeated cross-sectional analysis using supervised learning. The process is flexible enough to be applied to any domain. 

The findings of this case study also provided valuable new insights for researchers interested in Brazilian secondary education. Lastly, it should be noted that the preprocessed and standardized data used in this analysis is an additional contribution of this thesis and is available \cite{SilvaFilho2022EnemCensus2009-2019} for other researchers interested in the quantitative analysis of Brazilian secondary education.
  

\section{Future works}

As the use of \gls{ML} increases, so does the demand for interpretability, making \gls{XAI} a rapidly growing field with numerous new interpretation methods being introduced. In this thesis, rather than developing a new method, the focus is on applying an established method, \gls{ALE}, to the context of reporting global feature contributions in the educational domain. This approach aims to deepen and extend our understanding of its potential to enhance the interpretability of educational models.

The contribution of this thesis can be improved and further explored in future works. For instance, the benchmarking process of Chapter \ref{chap:AssALE} could be expanded to include more algorithms as well as a wider range of data scenarios, including the presence of outliers and missing values. Similarly, these variations could be applied to the evaluation of the new metrics of Chapter \ref{chapter4}. Specifically, while the potential of these metrics as a feature selection method was presented, detailed scrutiny was beyond the scope of this thesis. Consequently, there remains a need for empirical evidence to establish their effectiveness fully in this direction.

Moreover, from a broader perspective, by concentrating on using \gls{XAI} in \gls{EDM} to obtain global explanations, I believe that investigating the following related topics could substantially advance the field. 

\subsection{True to the model, true to the data, and true to the context}

A central challenge in explainable methods lies in balancing fidelity to the model and the data. This tension forms a core part of this thesis's motivation. Overemphasis on the model can lead to unreliable explanations due to neglect of data relationships. Conversely, focusing solely on data may preclude leveraging complex functions that fit the data \gls{ALE} have emerged as a promising solution to this dilemma, especially in the context of supervised learning for knowledge extraction from data. However, we argue there is another perspective that new \gls{XAI} techniques must be aware of in education: the context perspective.

Contextual understanding involves comprehending how features semantically affect observations. While \gls{ALE} and other global techniques effectively identify varying feature effects across their value range, there is limited exploration in contextualizing which observations correspond to each feature value. A potential breakthrough could be a technique that uncovers heterogeneous feature effects, pinpointing relevant groups based on their distinct responses to a feature within the model. This approach aligns with the existing literature on model fairness in \gls{EDM}, which aims to identify the poor performance of \gls{ML} models in sensitive groups. Viewing it through the lens of \gls{XAI} at the feature level could significantly enhance the utility of \gls{EDM} in providing valuable insights from educational data.

\subsection{Data dependence is the real world}

Historically, traditional linear models have been the most widely used method for extracting knowledge from data in the education domain. When specifying these models, the interpretation of coefficients is seen as the effect of a variable on the dependent variable. This perspective has guided researchers in supervised learning, with the primary aim of harnessing \gls{ML}'s powerful pattern recognition capabilities while maintaining a level of interpretability that tries to mimic traditional statistical models. Within this context, this thesis endeavors to introduce alternatives that better address the complexity of data dependence on educational datasets, dealing with the trade-off between being true to the data and true to the model.

However, a different approach to dealing with data dependence, treating it as an inherent part of the \gls{ML} paradigm and also from the real world, can extend the meaningfulness of \gls{XAI} in \gls{EDM}. \gls{ML} is inherently associative, and this property can be leveraged to gain deeper insights into the data-generating process. Instead of focusing solely on isolating the effects of individual features or their interactions, \gls{ML} allows researchers to explore the network of relationships within the data. This approach recognizes the complexity and interconnectedness of educational environments as part of the problem. It enables a more holistic view, shifting the focus from measuring isolated feature effects to understanding the network of relationships within the data. For example, this could involve exploring how and when different aspects of the school environment interact with student backgrounds or how policy changes ripple through various layers of the education system.

\subsection{Beyond a one-size-fits-all }

This thesis has established the \gls{ALE} as a robust \gls{XAI} technique for reporting global feature contributions, particularly in the context of dependent data. While the potential of \gls{ALE} is established, and the \gls{ALE} framework has been further explored in this study in order to enhance data interpretability, it is important to recognize that \gls{XAI} in \gls{DM} and \gls{ML} is inherently exploratory. No single method uniformly suits all scenarios. The efficacy of combining techniques, both for complementary insights and ensemble approaches, presents a significant area for exploration. This concept has been demonstrated in \cite{Fisher2018AllSimultaneously}, where multiple models were utilized to generate more reliable scores. Furthermore, the adoption of frameworks that integrate various techniques aiming for more model-specific explanations has shown promise, as discussed in \cite{Li2019AForests}. We believe that the synergistic application of multiple \gls{XAI} techniques and paradigms, particularly aimed at enhancing the quality of insights in educational contexts, represents a promising direction for future research.




 




