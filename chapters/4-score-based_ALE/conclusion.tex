\section{Summary}

This chapter introduces four novel metrics - \gls{MUA}, \gls{UAS}, \gls{AUA},  and \gls{AAR} - derived from \gls{ALE} framework that quantify the significance of features within predictive models. Each metric provides a clear definition that captures a specific aspect of feature relevance. Although primarily driven by the limitations of existing metrics in \gls{EDM}, which frequently overlook the contributions of features in datasets with interrelated dependent variables, these new metrics also enhance the overall interpretability of supervised learning models. The experimental evaluation demonstrates the enhanced robustness of the \gls{ALE}-based framework in such contexts when compared to established methodologies. Specifically, a first round of experiments illustrates the ability of the new metrics to effectively identify critical features in synthetic datasets engineered with varying generating functions and degrees of feature interdependence. These new metrics either surpass or match the performance of existing baseline measures. Specifically, the \gls{ALE}-based metric generally yielded better results compared to \gls{SHAP} and \gls{MDI}, as they do not attribute significance to features that are irrelevant yet highly correlated with another relevant feature. Furthermore, they also outperformed \gls{PFI} and \gls{csPFI} in identifying multiple important features that are also correlated among them. When evaluating both properties, \gls{ALE} metrics achieve better results.  

We focus on \gls{RF} and \gls{NN}. Both models demonstrated good performance, with an \gls{RMSE} close to the standard deviation of the theoretical noise added to the target variable, indicating that the models were capable of detecting underlying patterns

Further, empirical evidence from real-world datasets corroborates the findings from synthetic experiments, showcasing the \gls{ALE}-based metrics' capacity to discern the main effects of variables. In a qualitative experiment utilizing an educational dataset and domain expertise to assess the expected relevance of key variables, \gls{ALE}-based scores yielded better results than the baseline. They accurately identified and isolated the relevance of variables without attributing significance to potentially irrelevant features solely due to their correlation with relevant ones.

Moreover, the chapter discusses the potential pitfalls of using \gls{SHAP}-based scores, particularly when computed through the computationally efficient TreeShap method for tree-based algorithms. It can lead to misleading interpretations due to its constraints to avoid extrapolation based on the tree structure of the explained model.

Finally, the potential of the proposal for feature selection and dimensional reduction tasks was demonstrated using openly real-world datasets, including one from the educational domain and two high-dimensional datasets which are commonly used in feature selection benchmarking. The experiments showcased the \gls{ALE} framework as a reliable, model-agnostic alternative for this purpose, capable of reducing model complexity with minimal performance degradation comparable to the baseline, sometimes achieving significantly better results. Specifically, the ALE-based score achieves better or comparable results when contrasted with the widely used feature importance of random forest (\gls{MDI}) in various scenarios. The ALE could remove irrelevant variables while increasing or keeping the model performance even in scenarios inherently biased in favor of the baseline.
