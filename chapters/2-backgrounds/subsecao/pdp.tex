\subsubsection{PD plots and scores}
\label{subpdp}

The Partial Dependence (\gls{PD}) plots serve as a graphical representation that quantifies the effect of specific features on the predicted outcome within a supervised learning model while holding other variables constant (\textit{ceteris paribus}). These plots offer insights into the average marginal contribution of a feature of interest $X_s$ to the model's prediction, with the remaining features $X_c$ held constant. By doing so, if the predictive model closes the real world, \gls{PD}s allow a causal interpretation of the role of $X_s$ in the model if data meets the independence assumption \cite{Zhao2021CausalModels}. The underlying function can be mathematically described as follows:

\begin{equation}
PD(X_s)=\frac{1}{n}\sum_{i=1}^{n}f(X_s=j, X_{c})
\label{pd}
\end{equation}

where $f(X_s = j, X_{c})$  represents the model's predicted output when the feature  $X_s$ is intervened upon to assume a specific value $j$, while the remaining features $X_{c}}$ are held their observed values in the dataset. The value $j$ is drawn from the marginal distribution of $X_s$. To be plotted, $j$ assumes values within a defined grid of the ordered $X_s$ where \ref{pd} is computed. For categorical features, $j$ assumes each category as a possible value.

A more specific method for estimating PD is utilizing Individual Conditional Expectation (ICE) curves. ICE curves \cite{Goldstein2015PeekingExpectation} provide a distinct curve $PD(X_s)$ for each individual data point $i$ in the sample. Essentially, the PD is computed as the average of these ICE curves. This granular decomposition facilitated by ICE allows for identifying potential interaction effects between $X_s$ and the remaining features $X_c$ at global level, which may not be observed when solely relying on \gls{PD}s. 

In an attempt to yield scores from \gls{PD}, \cite{Greenwell2018AMeasure} proposed a simple score considering that a feature's importance is inversely related to the flatness of its \gls{PD} Plot; a flatter \gls{PD} plot suggests lesser importance, while greater variation in the \gls{PD} indicates higher significance. Ass \gls{PD} ignores feature relationships, this \gls{PD}-based score captures only the main effect of the feature and ignores potential feature interactions

